import os
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from transformers import pipeline
from dotenv import load_dotenv

# Load environment variables from .env file (for sensitive info like API tokens)
load_dotenv()
HF_TOKEN = os.getenv("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")  # Use the environment variable for Hugging Face token

# Initialize Hugging Face embeddings model (sentence transformer model for document embeddings)
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)  # Load pre-trained embeddings model for document processing

# Initialize Hugging Face Q&A pipeline with DistilBERT fine-tuned on SQuAD
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad", tokenizer="distilbert-base-cased")

# Function to load documents from a specified directory
def load_documents(directory):
    documents = []  # List to store loaded documents
    # Loop through each file in the given directory
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)  # Full file path
        print(f"üìÑ Processing file: {file_path}")

        # If the file is a text file, load it using TextLoader
        if filename.endswith(".txt"):
            loader = TextLoader(file_path)
        # If the file is a PDF, load it using PyPDFLoader
        elif filename.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        else:
            # Skip unsupported file types
            print(f"‚ö†Ô∏è Skipping unsupported file: {filename}")
            continue
        
        # Load the content from the file and extend the documents list
        documents.extend(loader.load())
    
    return documents  # Return the list of loaded documents

# Function to create embeddings for the documents and store them in a FAISS index
def create_embedding_index(documents):
    if not documents:
        print("‚ùå No documents found!")  # If no documents were found, print an error
        return None

    # Split documents into smaller chunks to make them manageable for embedding processing
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)  # Split the documents into chunks
    text_data = [doc.page_content for doc in texts]  # Extract the content from each chunk

    print(f"‚úÖ Loaded {len(text_data)} text chunks.")  # Print how many chunks were loaded

    # Create a FAISS index from the embedded text chunks
    vector_db = FAISS.from_texts(text_data, embeddings)  # Use FAISS to store the embeddings and make them searchable
    return vector_db  # Return the FAISS index

# Function to retrieve the most relevant document chunks based on user query
def retrieve_documents(query, index, top_k=3):
    if index is None:
        print("‚ùå No valid FAISS index found.")  # If no valid index exists, return an error
        return []
    
    # Perform similarity search on the FAISS index to find relevant documents
    docs = index.similarity_search(query, k=top_k)  # Retrieve top-k most relevant chunks
    return [doc.page_content for doc in docs]  # Return the content of the most relevant chunks

# Function to generate a response using the Hugging Face Q&A pipeline
def generate_response(query, retrieved_docs):
    # Combine the retrieved document chunks into one string to use as context for the Q&A model
    context = "\n".join(retrieved_docs)
    input_data = {
        'question': query,  # The question the user has asked
        'context': context   # The relevant context (document chunks) retrieved from the FAISS index
    }
    
    # Pass the question and context to the Q&A pipeline and get the model's answer
    result = qa_pipeline(input_data)  # Run the question-answering pipeline
    return result['answer']  # Return the answer generated by the model

# Main function to load documents, create embeddings, and start the Q&A loop
def main():
    print("üöÄ AI-Powered Q&A System Starting...")
    
    # Load and process documents from the "docs" folder
    docs = load_documents("docs")
    
    # Create the embedding index using FAISS
    index = create_embedding_index(docs)

    # Start the Q&A loop for user interaction
    while True:
        query = input("\nüîé Enter your question (or 'exit' to quit): ")  # Get user input (question)
        if query.lower() == "exit":
            break  # Exit the loop if the user types 'exit'
        
        # Retrieve relevant documents from the FAISS index based on the user's question
        retrieved_docs = retrieve_documents(query, index)
        
        # Generate an answer using the retrieved documents
        answer = generate_response(query, retrieved_docs)

        print("\n‚úÖ AI Answer:", answer)  # Print the AI-generated answer

# Entry point of the script when it is run directly
if __name__ == "__main__":
    main()  # Start the AI-powered Q&A system
